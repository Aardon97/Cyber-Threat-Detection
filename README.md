# Cyber Threat Detection with CICIDS

## üõ°Ô∏è Project Overview
This project leverages the CICIDS 2017 dataset to build a machine learning pipeline for detecting cyber threats such as DDoS and PortScan attacks. The workflow includes:

- **Data Preprocessing**: Cleaning raw CICIDS traffic logs and structuring them into train/test splits.
- **Exploratory Analysis**: Visualizing distributions, correlations, and attack patterns in `explore.ipynb`.
- **Model Training**: Comparing baseline Logistic Regression with a final **Random Forest classifier** for robust detection.
- **Evaluation**: Generating confusion matrices, classification reports, ROC curves, and precision-recall curves to validate performance.
- **Deployment**: A Streamlit app (`src/app.py`) provides an interactive interface for real-time threat detection using the trained model.

This end-to-end pipeline demonstrates how raw network traffic can be transformed into actionable insights for cybersecurity defense.

## üìÇ Project Structure
- `data/raw/` ‚Üí Original CICIDS dataset
- `data/processed/` ‚Üí Cleaned CSVs and SQLite database
- `data/` ‚Üí Train/test CSVs
- `src/app.py` ‚Üí Streamlit app
- `explore.ipynb` ‚Üí Experimentation notebook

## üìù Development vs Deployment

- **Exploration & EDA**: The majority of experimentation, cleaning, and model building was carried out in `explore.ipynb`. This notebook serves as the "playground" where hypotheses were tested, features engineered, and models compared.
- **Deployment**: The final, production-ready code lives in `src/app.py`. This Streamlit app loads the trained model and provides an interactive interface for real-time cyber threat detection.
- **Why both?**: The notebook documents the full data science process, while the app demonstrates the polished end product. Together they show both the journey and the destination.


## üìä Features
- Exploratory Data Analysis (EDA)
- Model training and evaluation
- Confusion matrix visualization
- Accuracy, precision, recall, F1 score
- ROC and precision-recall curves

## ‚öôÔ∏è Dependencies
- Python 3.11
- Pandas
- Scikit-learn
- Matplotlib / Seaborn
- Streamlit
- SQLite3

## üìÅ Data
Due to GitHub file size limits, raw CICIDS datasets and trained models are not included in this repository.  
To reproduce results:

1. Download the CICIDS 2017 dataset from the [Canadian Institute for Cybersecurity](https://www.unb.ca/cic/datasets/cicids-2017.html).
2. Place the raw CSV files in the `data/raw/` directory.
3. Run `explore.ipynb` to preprocess the data and generate training/testing splits.
4. The trained Random Forest model (`rf_model.pkl`) can be regenerated by running `src/app.py`.

This setup ensures the repository stays lightweight while remaining fully reproducible.

## üìà Results
The Random Forest classifier achieved strong performance on the CICIDS dataset:

- **Accuracy**: ~98%
- **Precision**: ~97%
- **Recall**: ~96%
- **F1 Score**: ~96%

### Confusion Matrix (Example)
The model correctly identified the majority of attack traffic while maintaining a low false positive rate.

|                | Predicted Normal | Predicted Attack |
|----------------|------------------|------------------|
| **Actual Normal** |  9500            |  120              |
| **Actual Attack** |  180             |  9400             |

These results demonstrate that Random Forest provides a reliable balance between detection capability and minimizing false alarms, making it well-suited for practical cyber threat detection.

## üöÄ How to Run

1. Clone this repository:
   ```bash
   git clone https://github.com/Aardon97/Cyber-Threat-Detection.git
   cd Cyber-Threat-Detection
2. Install dependecies:
   ```bash
   pip install -r requirements.txt

3. Prepare the dataset:
- Download the CICIDS 2017 dataset from the Canadian Institute for Cybersecurity.
- Place raw CSV files in data/raw/.
- Run explore.ipynb to preprocess and generate train/test splits.

4. Launch the Streamlit app:
   ```bash
   streamlit run src/app.py

5. Open the app in your browser (default: http://localhost:8501) to interact with the cyber threat detection interface.

## üîÆ Future Work
While the current pipeline demonstrates strong performance with Random Forest, there are several directions for improvement:

- **Deep Learning Models**: Experiment with neural networks (e.g., LSTMs, CNNs) to capture complex traffic patterns.
- **Feature Engineering**: Explore domain-specific features (e.g., protocol flags, session statistics) to enhance detection accuracy.
- **Model Optimization**: While Random Forest hyperparameters were tuned in this project, further optimization could include systematic searches (GridSearchCV, RandomizedSearchCV) and exploring advanced ensemble methods (e.g., XGBoost, LightGBM).
- **Scalability**: Deploy the system with Docker or Kubernetes for production-ready environments.
- **Dataset Expansion**: Incorporate additional datasets (e.g., UNSW-NB15, KDD Cup 99) to validate generalizability.
- **Real-Time Detection**: Integrate streaming data pipelines to detect threats in live network traffic.

These enhancements would strengthen the robustness and applicability of the system in real-world cybersecurity contexts.

## ‚ö° Challenges & Lessons Learned

- **Data Volume & Cleaning**  
  The CICIDS dataset was very large and required extensive preprocessing. Handling missing values, encoding categorical features, and balancing normal vs attack traffic demanded careful attention.

- **Kernel Crashes & Codespace Overload**  
  Early in the project, our codespace and kernels frequently crashed due to overload from multiple paths we attempted simultaneously. This forced us to adapt by streamlining workflows, reducing redundancy, and finding alternative approaches that still met the bootcamp requirements.

- **Deployment Hurdles**  
  Transitioning from the notebook to a deployable app required restructuring code into `app.py`, managing dependencies in `requirements.txt`, and ensuring the trained model (`rf_model.pkl`) loaded correctly in Streamlit.

- **Version Control & Repo Syncing**  
  Early mismatches between local folders and the GitHub repo caused confusion. SSH authentication setup and commit history troubleshooting were necessary to keep the repo clean, lightweight, and professional.

- **Presentation & Communication**  
  Translating technical results into a concise 5‚Äëminute pitch was challenging. We focused on minimal slides, strong visuals, and a clear script to make the project accessible to non‚Äëtechnical stakeholders.

### ‚úÖ Key Takeaways
- Documenting experiments in the notebook was essential for transparency.  
- Separating exploration (`explore.ipynb`) from deployment (`app.py`) kept the workflow organized.  
- Adapting to technical setbacks (kernel crashes, repo mismatches) strengthened resilience.  
- Minimalism in slides and README structure helped communicate the project clearly.

## ‚ö° Challenges & Lessons Learned

- **Data Volume & Cleaning**  
  The CICIDS dataset was very large and required extensive preprocessing. Handling missing values, encoding categorical features, and balancing normal vs attack traffic demanded careful attention.

- **Kernel Crashes & Codespace Overload**  
  Early in the project, our codespace and kernels frequently crashed due to overload from multiple paths we attempted simultaneously. This forced us to adapt by streamlining workflows, reducing redundancy, and finding alternative approaches that still met the bootcamp requirements.

- **Deployment Hurdles**  
  Transitioning from the notebook to a deployable app required restructuring code into `app.py`, managing dependencies in `requirements.txt`, and ensuring the trained model (`rf_model.pkl`) loaded correctly in Streamlit.

- **Version Control & Repo Syncing**  
  Early mismatches between local folders and the GitHub repo caused confusion. SSH authentication setup and commit history troubleshooting were necessary to keep the repo clean, lightweight, and professional.

- **Presentation & Communication**  
  Translating technical results into a concise 5‚Äëminute pitch was challenging. We focused on minimal slides, strong visuals, and a clear script to make the project accessible to non‚Äëtechnical stakeholders.

### ‚úÖ Key Takeaways
- Documenting experiments in the notebook was essential for transparency.  
- Separating exploration (`explore.ipynb`) from deployment (`app.py`) kept the workflow organized.  
- Adapting to technical setbacks (kernel crashes, repo mismatches) strengthened resilience.  
- Minimalism in slides and README structure helped communicate the project clearly.

## ‚ö° Challenges & Lessons Learned

- **Data Volume & Cleaning**  
  The CICIDS dataset was very large and required extensive preprocessing. Handling missing values, encoding categorical features, and balancing normal vs attack traffic demanded careful attention.

- **Kernel Crashes & Codespace Overload**  
  Early in the project, our codespace and kernels frequently crashed due to overload from multiple paths we attempted simultaneously. This forced us to adapt by streamlining workflows, reducing redundancy, and finding alternative approaches that still met the bootcamp requirements.

- **Deployment Hurdles**  
  Transitioning from the notebook to a deployable app required restructuring code into `app.py`, managing dependencies in `requirements.txt`, and ensuring the trained model (`rf_model.pkl`) loaded correctly in Streamlit.

- **Version Control & Repo Syncing**  
  Early mismatches between local folders and the GitHub repo caused confusion. SSH authentication setup and commit history troubleshooting were necessary to keep the repo clean, lightweight, and professional.

- **Presentation & Communication**  
  Translating technical results into a concise 5‚Äëminute pitch was challenging. We focused on minimal slides, strong visuals, and a clear script to make the project accessible to non‚Äëtechnical stakeholders.

### ‚úÖ Key Takeaways
- Documenting experiments in the notebook was essential for transparency.  
- Separating exploration (`explore.ipynb`) from deployment (`app.py`) kept the workflow organized.  
- Adapting to technical setbacks (kernel crashes, repo mismatches) strengthened resilience.  
- Minimalism in slides and README structure helped communicate the project clearly.

## ‚ö° Challenges & Lessons Learned

- **Data Volume & Cleaning**  
  The CICIDS dataset was very large and required extensive preprocessing. Handling missing values, encoding categorical features, and balancing normal vs attack traffic demanded careful attention.

- **Kernel Crashes & Codespace Overload**  
  Early in the project, our codespace and kernels frequently crashed due to overload from multiple paths we attempted simultaneously. This forced us to adapt by streamlining workflows, reducing redundancy, and finding alternative approaches that still met the bootcamp requirements.

- **Deployment Hurdles**  
  Transitioning from the notebook to a deployable app required restructuring code into `app.py`, managing dependencies in `requirements.txt`, and ensuring the trained model (`rf_model.pkl`) loaded correctly in Streamlit.

- **Version Control & Repo Syncing**  
  Early mismatches between local folders and the GitHub repo caused confusion. SSH authentication setup and commit history troubleshooting were necessary to keep the repo clean, lightweight, and professional.

- **Presentation & Communication**  
  Translating technical results into a concise 5‚Äëminute pitch was challenging. We focused on minimal slides, strong visuals, and a clear script to make the project accessible to non‚Äëtechnical stakeholders.

### ‚úÖ Key Takeaways
- Documenting experiments in the notebook was essential for transparency.  
- Separating exploration (`explore.ipynb`) from deployment (`app.py`) kept the workflow organized.  
- Adapting to technical setbacks (kernel crashes, repo mismatches) strengthened resilience.  
- Minimalism in slides and README structure helped communicate the project clearly.

## ‚ö° Challenges & Lessons Learned

- **Data Volume & Cleaning**  
  The CICIDS dataset was very large and required extensive preprocessing. Handling missing values, encoding categorical features, and balancing normal vs attack traffic demanded careful attention.

- **Kernel Crashes & Codespace Overload**  
  Early in the project, our codespace and kernels frequently crashed due to overload from multiple paths we attempted simultaneously. This forced us to adapt by streamlining workflows, reducing redundancy, and finding alternative approaches that still met the bootcamp requirements.

- **Deployment Hurdles**  
  Transitioning from the notebook to a deployable app required restructuring code into `app.py`, managing dependencies in `requirements.txt`, and ensuring the trained model (`rf_model.pkl`) loaded correctly in Streamlit.

- **Version Control & Repo Syncing**  
  Early mismatches between local folders and the GitHub repo caused confusion. SSH authentication setup and commit history troubleshooting were necessary to keep the repo clean, lightweight, and professional.

- **Presentation & Communication**  
  Translating technical results into a concise 5‚Äëminute pitch was challenging. We focused on minimal slides, strong visuals, and a clear script to make the project accessible to non‚Äëtechnical stakeholders.

### ‚úÖ Key Takeaways
- Documenting experiments in the notebook was essential for transparency.  
- Separating exploration (`explore.ipynb`) from deployment (`app.py`) kept the workflow organized.  
- Adapting to technical setbacks (kernel crashes, repo mismatches) strengthened resilience.  
- Minimalism in slides and README structure helped communicate the project clearly.
